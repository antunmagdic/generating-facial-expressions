\section{experimental Results}

Dataset: 
FER2013\footnote{\texttt{\url{https://www.kaggle.com/msambare/fer2013?select=train}}}. 
The data consists of $48\times48$ pixel grayscale images of faces. The faces 
have been automatically cropped so that the face is centered. Images are labeled
with 7 different emotions: Angry, Disgust, Fear, Happy, Sad, Surprise and 
Neutral. We used 4 of them to train our model due to lack of processing power: 
Angry (3395 images), Happy (7215 images), Sad (4830 images), Surprise (3171 
images). 

Training was performed using Adam optimizer \cite{kingma2014} with 
$\beta_1 = 0.5$ and $\beta_2 = 0.999$. We train our model with learning rate
0.0001 for 400 epochs. Batch size is set to 8. Training execution time was 
roughly 1 day on NVIDIA Tesla T4 GPU. 

Gradual improvement of desired facial expressions can be seen throughout epochs.
After 100 epochs, there are only slight changes which are prevalent in the mouth
area. Visible lines, which remind of moustache, appear around the mouth since it
is where facial expressions most differ. By the 400th epoch, modifications are 
more obvious and the desired emotions more easily recognizable. Changes involve 
several facial areas, including eyebrows, eyes, etc. 

Pronounced changes are visible on more expressive emotions such as
happiness, anger and surprise while changes on sad faces are somewhat less
apparent. 
