\section{Proposed Solution}

Our solution is the same as the one proposed in \cite{choi2018}. The StarGAN 
framework uses a single generator $G$ and a single discriminator $D$. 
Generator's role is to produce real images of desired class (in this case 
desired emotion) conditioned on the input image, while the discriminator is used
in the training process of the generator. We denote the output of the generator 
as $y = G(x, c)$, where $y$ is the generated image, $x$ is the input image and 
$c$ is the desired class. Out descriminator produces two probability 
distributions, one over sources and one over classes. We denote those as 
$D_{\mathrm{src}}(x)$ and $D_{\mathrm{cls}}(x)$.

This is an instance of multi objective optimization because the produced image 
has to contain the desired emotion, look real as well as similar to the original
image. This is reflected in our formulation of the loss function which contains 
multiple terms, each of which is used to optimize one aspect of the optimization
problem. We will consider each term separately and then combine them to 
formulate the total loss function for the generator as well as for the 
discriminator.

First term in the loss function is the standard adversarial loss
\begin{equation}
\begin{split}
\mathcal{L}_{\mathrm{adv}} = \;&
    \mathbb{E}_x \left[ \log D_{\mathrm{src}}(x)\right] + \\
    & \mathbb{E}_{x, c} \left[\log(1 - D_{\mathrm{src}}(G(x, c)))\right].
\end{split}
\end{equation}
The purpose of this term is to make images generated by the generator look real.
The generator tries to minimize this term while the descriminator tries to 
maximize it. Training the descriminator will in turn make the generator produce 
work harder to produce images that will be classified as real, therefore the 
quality of the generated images will increase.

Second term in the loss function is the classification loss. Since we want the 
generated images to belong to the desired class the classification loss term has
to be included in the total loss function. We can train the discriminator on the
real data and thus furmulate the loss
\begin{equation}
\label{eq:lclsr}
\mathcal{L}_\mathrm{cls}^\mathrm{r} = 
    \mathbb{E}_{x, c'} \left[-\log D_{\mathrm{cls}}(c'|x)\right].
\end{equation}
Since the generator also needs to learn the notion of different classes we can 
use almost the same loss to train the generator, therefore we formulate the loss
\begin{equation}
\mathcal{L}_\mathrm{cls}^\mathrm{f} = 
    \mathbb{E}_{x, c} \left[-\log D_{\mathrm{cls}}(c|G(x, c))\right].
\end{equation}
This expression is almost the same as the one in \eqref{eq:lclsr}. The only 
difference is that the real image $x$ from \eqref{eq:lclsr} is here replaced by 
the generated fake image $G(x, c)$. Both the generator and the discriminator 
want to minimize those losses, although for different reasons. The discriminator 
wants to get better at classifying, and the generator wants to get better at 
producing images that will fool the descriminator.

The previous two terms insure that generated images will look real and contain 
features of the desired class, but there is nothin so far that would stop the 
generator to learn only one image for each class and completely disregard the 
$x$ in $G(x, c)$. For this reason we include the third loss term: the 
reconstruction loss
\begin{equation}
\mathcal{L}_\mathrm{rec} = \mathbb{E}_{x, c, c'}\left[
    \lVert x - G(G(x, c), c')\lVert_1
\right],
\end{equation}
where $c$ is some class s.t. $c \neq c'$ and $c'$ is the true class of $x$. This
term insures that the generated image will preserve all the features invariant 
with respect to the class $c$ (in the case of emotions as classes, such features
would are e.g. hair color, eye color, nose type...). This is accomplished by 
applying the generator twice. First time to the image $x$ and with desired class
$c$, and second time to the generated image $G(x, c)$ and with the original 
image class $c'$. If the generator changes only features correlated with the 
class the reconstruction should be easy and the result of cyclical generator 
application should be almost the same as the original image $x$, for all the 
features orthogonal to class will remain unchanged.

The total loss functions for the generator and the discriminator are linear 
combinations of previously defined terms. The generator should minimize
\begin{equation}
\mathcal{L}_G = \mathcal{L}_\mathrm{adv} + 
    \lambda_\mathrm{cls} \mathcal{L}_\mathrm{cls}^\mathrm{f} + 
    \lambda_\mathrm{rec} \mathcal{L}_\mathrm{rec}
\end{equation}
and the discriminator should minimize
\begin{equation}
\mathcal{L}_D = -\mathcal{L}_\mathrm{adv} + 
    \lambda_\mathrm{cls} \mathcal{L}_\mathrm{cls}^\mathrm{r},
\end{equation}
where lambdas are hyperparameters whose variation changes the relative 
importance of various loss terms, e.g. setting $\lambda_\mathrm{cls} = 0$ would 
probably result in generator producing input images. As in \cite{choi2018} we 
use $\lambda_\mathrm{cls} = 1$ and $\lambda_\mathrm{rec} = 10$. Our generator 
and discriminator architectures are also the same as in \cite{choi2018}.
